Q-Learning is a widely used algorithm in reinforcement learning that enables an agent to learn an optimal policy for decision-making in an environment. It operates by estimating the value of each state-action pair, known as the Q-values, which represent the expected cumulative rewards an agent can obtain by taking a specific action in a particular state. The Q-Learning algorithm iteratively updates the Q-values based on the agent's experiences in the environment. 

During training, the agent explores the environment and interacts with it by taking actions, observing the resulting rewards, and transitioning to new states. The Q-values are updated using the Bellman equation, which expresses the relationship between the current Q-value and the Q-values of the next state and the best action to take in that state. Once the Q-values are learned, the agent can follow the policy that selects actions with the highest Q-values to make decisions and maximize cumulative rewards.

Q-Learning is an off-policy algorithm, meaning that it can learn from experiences generated by following a different policy. This property allows the algorithm to balance exploration and exploitation effectively. 