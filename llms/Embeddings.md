# Word and Sentence Embeddings

- Embeddings are vectors of numbers that capture the context, hierarchy and similarity of objects such as words, sentences and documents. 

- In practice, the spatial distances (and sometimes angles) between these vectors capture semantic relations like similarity, opposition, and more. The methods to infer similarity include: dot product, cosine similarity, and euclidean distance; which can be used to find articles that are contextually related but may not share exact keywords.

- Sentence embeddings can be extended to language embeddings, in which the numbers attached to each sentence are language-agnostic. These models are very useful for translation and for searching and understanding text in different languages.